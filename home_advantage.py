#!/usr/bin/env python3
"""
Analyze model performance by question source.
Shows how each model performs on questions generated by each other model.
"""

import json
import sys
from pathlib import Path

def load_json(filename, revision):
    """Load JSON file with revision tag."""
    path = Path("data") / f"{filename}_{revision}.json"
    if not path.exists():
        print(f"Error: {path} not found")
        sys.exit(1)
    with open(path, encoding="utf-8") as f:
        return json.load(f)

def analyze_by_source(revision="V3", mode="shuffle_blind"):
    """Analyze model performance grouped by question source."""

    # Load data
    phase1 = load_json("phase1_questions", revision)
    phase3 = load_json("phase3_rankings", revision)

    # Build question -> source model mapping
    question_source = {}
    for model, questions in phase1.get("questions_by_model", {}).items():
        for q in questions:
            question_source[q["question"]] = model

    # Get evaluations for the specified mode
    evaluations = phase3.get("evaluations_by_mode", {}).get(mode, {})
    if not evaluations:
        evaluations = phase3.get("evaluations", {})  # fallback

    # Collect scores: scores_by_source[answering_model][source_model] = [scores]
    scores_by_source = {}
    all_models = set()
    all_sources = set()

    for evaluator, evals in evaluations.items():
        for question, model_scores in evals.items():
            source = question_source.get(question)
            if not source:
                continue
            all_sources.add(source)

            for answering_model, data in model_scores.items():
                if evaluator == answering_model:
                    continue  # Exclude self-evaluations
                all_models.add(answering_model)

                if answering_model not in scores_by_source:
                    scores_by_source[answering_model] = {}
                if source not in scores_by_source[answering_model]:
                    scores_by_source[answering_model][source] = []

                score = data.get("score", 0)
                scores_by_source[answering_model][source].append(score)

    # Compute averages
    avg_by_source = {}
    for model in sorted(all_models):
        avg_by_source[model] = {}
        for source in sorted(all_sources):
            scores = scores_by_source.get(model, {}).get(source, [])
            if scores:
                avg_by_source[model][source] = sum(scores) / len(scores)
            else:
                avg_by_source[model][source] = None

    return avg_by_source, sorted(all_models), sorted(all_sources)

def short_name(model, max_len=12):
    """Shorten model name for display."""
    shortcuts = {
        "gemini-3-pro-preview": "gem-3-pro",
        "gemini-3-flash-thinking": "gem-3-flash",
        "claude-opus-4-5": "opus-4.5",
        "claude-sonnet-4-5": "sonnet-4.5",
        "llama-4-maverick": "llama-4",
        "deepseek-chat": "deepseek",
        "kimi-k2-0905": "kimi",
        "grok-4-1-fast": "grok-4",
        "mistral-large": "mistral",
    }
    return shortcuts.get(model, model)[:max_len]

def print_matrix(avg_by_source, models, sources):
    """Print the performance matrix."""
    # Header
    header = "Answerer\\Source"
    col_width = 10
    print(f"\n{'='*80}")
    print("PERFORMANCE BY QUESTION SOURCE (Peer scores, excluding self-evaluation)")
    print(f"{'='*80}\n")

    # Column headers (question sources)
    print(f"{'Answerer':<14}", end="")
    for src in sources:
        print(f" {short_name(src):>{col_width}}", end="")
    print(f" {'Overall':>{col_width}}")
    print("-" * (14 + (col_width + 1) * (len(sources) + 1)))

    # Rows (answering models)
    for model in models:
        print(f"{short_name(model):<14}", end="")
        row_scores = []
        for src in sources:
            score = avg_by_source[model].get(src)
            if score is not None:
                print(f" {score:>{col_width}.2f}", end="")
                row_scores.append(score)
            else:
                print(f" {'N/A':>{col_width}}", end="")

        # Overall average
        if row_scores:
            overall = sum(row_scores) / len(row_scores)
            print(f" {overall:>{col_width}.2f}")
        else:
            print(f" {'N/A':>{col_width}}")

    # Source averages (how hard are each model's questions?)
    print("-" * (14 + (col_width + 1) * (len(sources) + 1)))
    print(f"{'Q Difficulty':<14}", end="")
    for src in sources:
        src_scores = [avg_by_source[m].get(src) for m in models if avg_by_source[m].get(src)]
        if src_scores:
            avg = sum(src_scores) / len(src_scores)
            print(f" {avg:>{col_width}.2f}", end="")
        else:
            print(f" {'N/A':>{col_width}}", end="")
    print()

def collect_raw_scores_by_source(evaluations, question_source):
    """Collect raw individual scores grouped by (answering_model, source_model)."""
    raw_scores = {}  # raw_scores[answering_model][source_model] = [scores]

    for evaluator, evals in evaluations.items():
        for question, model_scores in evals.items():
            source = question_source.get(question)
            if not source:
                continue

            for answering_model, data in model_scores.items():
                if evaluator == answering_model:
                    continue  # Exclude self-evaluations

                if answering_model not in raw_scores:
                    raw_scores[answering_model] = {}
                if source not in raw_scores[answering_model]:
                    raw_scores[answering_model][source] = []

                score = data.get("score", 0)
                raw_scores[answering_model][source].append(score)

    return raw_scores


def cohens_d(group1, group2):
    """Calculate Cohen's d effect size."""
    import math
    n1, n2 = len(group1), len(group2)
    if n1 < 2 or n2 < 2:
        return None

    mean1 = sum(group1) / n1
    mean2 = sum(group2) / n2

    var1 = sum((x - mean1) ** 2 for x in group1) / (n1 - 1)
    var2 = sum((x - mean2) ** 2 for x in group2) / (n2 - 1)

    # Pooled standard deviation
    pooled_std = math.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))

    if pooled_std == 0:
        return None

    return (mean1 - mean2) / pooled_std


def welch_ttest(group1, group2):
    """Perform Welch's t-test (unequal variances). Returns t-statistic and p-value."""
    import math

    n1, n2 = len(group1), len(group2)
    if n1 < 2 or n2 < 2:
        return None, None

    mean1 = sum(group1) / n1
    mean2 = sum(group2) / n2

    var1 = sum((x - mean1) ** 2 for x in group1) / (n1 - 1)
    var2 = sum((x - mean2) ** 2 for x in group2) / (n2 - 1)

    se = math.sqrt(var1 / n1 + var2 / n2)
    if se == 0:
        return None, None

    t_stat = (mean1 - mean2) / se

    # Welch-Satterthwaite degrees of freedom
    num = (var1 / n1 + var2 / n2) ** 2
    denom = (var1 / n1) ** 2 / (n1 - 1) + (var2 / n2) ** 2 / (n2 - 1)
    if denom == 0:
        return t_stat, None
    df = num / denom

    # Approximate p-value using t-distribution (two-tailed)
    # Using simple approximation for |t| > 2
    p_value = t_distribution_p(abs(t_stat), df)

    return t_stat, p_value


def t_distribution_p(t, df):
    """Approximate two-tailed p-value for t-distribution."""
    import math

    # Use normal approximation for large df
    if df > 100:
        # Standard normal approximation
        z = t
        p = 2 * (1 - normal_cdf(abs(z)))
        return p

    # Beta function approximation for smaller df
    x = df / (df + t * t)
    # Incomplete beta function approximation
    p = regularized_incomplete_beta(df / 2, 0.5, x)
    return p


def normal_cdf(x):
    """Approximation of standard normal CDF."""
    import math
    return 0.5 * (1 + math.erf(x / math.sqrt(2)))


def regularized_incomplete_beta(a, b, x):
    """Simple approximation for regularized incomplete beta function."""
    import math

    # Use continued fraction or series expansion
    # For our use case (t-test p-values), we use a simpler approach
    if x == 0:
        return 0
    if x == 1:
        return 1

    # Simple numerical integration (trapezoidal)
    n_steps = 1000
    dx = x / n_steps
    total = 0
    for i in range(n_steps):
        xi = (i + 0.5) * dx
        if xi > 0 and xi < 1:
            try:
                total += xi ** (a - 1) * (1 - xi) ** (b - 1)
            except:
                pass
    total *= dx

    # Normalize by beta function
    try:
        beta_ab = math.gamma(a) * math.gamma(b) / math.gamma(a + b)
        return total / beta_ab
    except:
        return 0.5  # fallback


def analyze_self_question_performance(raw_scores, models, sources):
    """Analyze if models perform better/worse on their own questions with statistics."""
    print(f"\n{'='*80}")
    print("SELF-QUESTION PERFORMANCE (Do models excel on their own questions?)")
    print(f"{'='*80}\n")

    results = []
    for model in models:
        if model not in sources or model not in raw_scores:
            continue

        # Raw scores on own questions
        own_scores = raw_scores[model].get(model, [])
        if len(own_scores) < 5:
            continue

        # Raw scores on others' questions
        other_scores = []
        for src in sources:
            if src != model and src in raw_scores[model]:
                other_scores.extend(raw_scores[model][src])

        if len(other_scores) < 5:
            continue

        own_avg = sum(own_scores) / len(own_scores)
        other_avg = sum(other_scores) / len(other_scores)
        diff = own_avg - other_avg

        # Statistical tests
        t_stat, p_value = welch_ttest(own_scores, other_scores)
        d = cohens_d(own_scores, other_scores)

        results.append({
            "model": model,
            "own_avg": own_avg,
            "other_avg": other_avg,
            "diff": diff,
            "n_own": len(own_scores),
            "n_other": len(other_scores),
            "t_stat": t_stat,
            "p_value": p_value,
            "cohens_d": d,
        })

    # Sort by difference
    results.sort(key=lambda x: x["diff"], reverse=True)

    # Print header
    print(f"{'Model':<12} {'Own':>6} {'Other':>6} {'Diff':>7} {'n_own':>6} {'n_oth':>6} {'t':>7} {'p-val':>8} {'d':>6}  Sig")
    print("-" * 95)

    for r in results:
        # Significance stars
        if r["p_value"] is None:
            sig = "?"
        elif r["p_value"] < 0.001:
            sig = "***"
        elif r["p_value"] < 0.01:
            sig = "**"
        elif r["p_value"] < 0.05:
            sig = "*"
        else:
            sig = ""

        # Effect size interpretation
        d_str = f"{r['cohens_d']:+.2f}" if r["cohens_d"] else "N/A"
        p_str = f"{r['p_value']:.4f}" if r["p_value"] else "N/A"
        t_str = f"{r['t_stat']:+.2f}" if r["t_stat"] else "N/A"

        print(f"{short_name(r['model']):<12} {r['own_avg']:>6.2f} {r['other_avg']:>6.2f} "
              f"{r['diff']:>+7.2f} {r['n_own']:>6} {r['n_other']:>6} "
              f"{t_str:>7} {p_str:>8} {d_str:>6}  {sig}")

    # Summary
    print("\n" + "-" * 95)
    print("Effect size (Cohen's d): |d|<0.2 negligible, 0.2-0.5 small, 0.5-0.8 medium, >0.8 large")
    print("Significance: * p<0.05, ** p<0.01, *** p<0.001")

    sig_positive = sum(1 for r in results if r["p_value"] and r["p_value"] < 0.05 and r["diff"] > 0)
    sig_negative = sum(1 for r in results if r["p_value"] and r["p_value"] < 0.05 and r["diff"] < 0)
    not_sig = len(results) - sig_positive - sig_negative

    print(f"\nStatistically significant: {sig_positive} better on own, {sig_negative} worse on own, {not_sig} not significant")

    # Overall effect
    all_diffs = [r["diff"] for r in results]
    all_d = [r["cohens_d"] for r in results if r["cohens_d"]]
    if all_diffs:
        print(f"Average home advantage: {sum(all_diffs)/len(all_diffs):+.3f} points")
    if all_d:
        print(f"Average effect size: d={sum(all_d)/len(all_d):+.3f}")


def find_insights(avg_by_source, models, sources):
    """Find notable patterns in the data."""
    print(f"\n{'='*80}")
    print("KEY INSIGHTS")
    print(f"{'='*80}\n")

    # 1. Which source's questions are hardest/easiest?
    source_avg = {}
    for src in sources:
        scores = [avg_by_source[m].get(src) for m in models if avg_by_source[m].get(src)]
        if scores:
            source_avg[src] = sum(scores) / len(scores)

    sorted_sources = sorted(source_avg.items(), key=lambda x: x[1])
    print("HARDEST question sources (lowest avg peer scores):")
    for src, avg in sorted_sources[:3]:
        print(f"  {short_name(src)}: {avg:.2f}")

    print("\nEASIEST question sources (highest avg peer scores):")
    for src, avg in sorted_sources[-3:][::-1]:
        print(f"  {short_name(src)}: {avg:.2f}")

    # 2. For each model, where do they excel vs struggle?
    print("\n" + "-"*40)
    print("MODEL STRENGTHS/WEAKNESSES by question source:\n")

    for model in models:
        model_scores = [(src, avg_by_source[model].get(src))
                       for src in sources if avg_by_source[model].get(src)]
        if not model_scores:
            continue

        model_scores.sort(key=lambda x: x[1])
        worst = model_scores[0]
        best = model_scores[-1]
        overall = sum(s for _, s in model_scores) / len(model_scores)

        # Only show if there's meaningful variance
        if best[1] - worst[1] > 0.3:
            print(f"{short_name(model):12} overall={overall:.2f}  "
                  f"best={short_name(best[0])}({best[1]:.2f})  "
                  f"worst={short_name(worst[0])}({worst[1]:.2f})  "
                  f"spread={best[1]-worst[1]:.2f}")

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Analyze performance by question source")
    parser.add_argument("--revision", "-r", default="V3", help="Revision tag (default: V3)")
    parser.add_argument("--mode", "-m", default="shuffle_blind",
                       choices=["shuffle_blind", "shuffle_only", "blind_only"],
                       help="Evaluation mode (default: shuffle_blind)")
    args = parser.parse_args()

    avg_by_source, models, sources = analyze_by_source(args.revision, args.mode)
    print_matrix(avg_by_source, models, sources)

    # Load raw data for statistical analysis
    phase1 = load_json("phase1_questions", args.revision)
    phase3 = load_json("phase3_rankings", args.revision)

    # Build question -> source mapping
    question_source = {}
    for model, questions in phase1.get("questions_by_model", {}).items():
        for q in questions:
            question_source[q["question"]] = model

    # Get evaluations
    evaluations = phase3.get("evaluations_by_mode", {}).get(args.mode, {})
    if not evaluations:
        evaluations = phase3.get("evaluations", {})

    # Collect raw scores for statistical analysis
    raw_scores = collect_raw_scores_by_source(evaluations, question_source)

    analyze_self_question_performance(raw_scores, models, sources)
    find_insights(avg_by_source, models, sources)

if __name__ == "__main__":
    main()
