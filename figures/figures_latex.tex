%% PeerRank Figures - LaTeX Templates
%% Auto-generated by generate_figures.py
%% Copy these into your Overleaf document
%%
%% FIGURE ORGANIZATION:
%% - Fig 1: Pipeline diagram (user-created, in Methodology)
%% - Figs 3-5: Results section
%% - Figs 6-9: Discussion section (Bias Analysis)

\usepackage{graphicx}
\usepackage{subcaption}  % For subfigures

%% ============================================================================
%% METHODOLOGY SECTION
%% ============================================================================
%% Figure 1: Pipeline diagram (user-created, not auto-generated)
%% \begin{figure}[htbp]
%%     \centering
%%     \includegraphics[width=\linewidth]{figures/fig1_pipeline.pdf}
%%     \caption{Fully endogenous evaluation pipeline...}
%%     \label{fig:pipeline}
%% \end{figure}

%% ============================================================================
%% RESULTS SECTION
%% ============================================================================

%% FIGURE 2: Peer Rankings (Main Result)
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig4_peer_rankings.pdf}
    \caption{Peer evaluation rankings of seven large language models. Bars represent mean peer scores (excluding self-ratings) on a 1--10 scale, with error bars indicating standard deviation. Models are ranked by peer consensus.}
    \label{fig:peer-rankings}
\end{figure}

%% FIGURE 3: Cross-Evaluation Heatmap
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig5_cross_eval_heatmap.pdf}
    \caption{Cross-evaluation matrix showing average scores assigned by each evaluator model (rows) to each evaluated model (columns). Diagonal cells (highlighted) represent self-ratings. Color scale ranges from red (low scores) to green (high scores), centered at 7.5.}
    \label{fig:cross-eval}
\end{figure}

%% FIGURE 4: Peer Score vs Speed Trade-off
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig6_peer_score_vs_time.pdf}
    \caption{Peer score versus response time trade-off across evaluated models. The upper-left quadrant represents optimal performance (high peer score, low latency). Dashed lines indicate median values for each axis.}
    \label{fig:peer-score-speed}
\end{figure}


%% ============================================================================
%% DISCUSSION SECTION - Bias Analysis
%% ============================================================================

%% FIGURE 10: Self Bias
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig10_self_bias.pdf}
    \caption{Self bias across models. Bars show the difference between self-ratings and peer ratings (Self $-$ Peer). Positive values (orange) indicate models that overrate their own responses; negative values (blue) indicate models that underrate themselves.}
    \label{fig:self-bias}
\end{figure}

%% FIGURE 11: Name Bias
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig11_name_bias.pdf}
    \caption{Name bias across models. Bars show score change when model identity is revealed (Shuffle $-$ Peer). Positive values (green) indicate brand recognition helped; negative values (red) indicate name hurt scores.}
    \label{fig:name-bias}
\end{figure}

%% FIGURE 12: Position Bias
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig12_position_bias.pdf}
    \caption{Position bias by presentation order. Bars show score change due to fixed position in blind evaluation (Blind $-$ Peer). Positive values indicate the position helped; negative values indicate the position hurt scores.}
    \label{fig:position-bias}
\end{figure}

%% FIGURE 13: Judge Generosity
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig13_judge_generosity.pdf}
    \caption{Judge generosity across models. Bars show average score given by each model when evaluating peers, with error bars indicating standard deviation. Higher values indicate more lenient judging; the dashed line marks the overall mean.}
    \label{fig:judge-generosity}
\end{figure}

%% FIGURE 14: Judge Generosity vs Peer Ranking
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig14_judge_generosity_vs_peer.pdf}
    \caption{Relationship between peer ranking and judge generosity. Each point represents a model (numbered by rank), showing its performance (peer score) on the x-axis and its judging leniency (average score given) on the y-axis.}
    \label{fig:judge-vs-peer}
\end{figure}

%% FIGURE 15: Judge Agreement Matrix
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/fig15_judge_agreement_matrix.pdf}
    \caption{Judge agreement matrix showing pairwise Pearson correlation between models' scoring patterns. Higher values (green) indicate judges that rate responses similarly; lower values (red) indicate divergent evaluation criteria.}
    \label{fig:judge-agreement}
\end{figure}

%% FIGURE 16: Question Autopsy
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/fig16_question_autopsy.pdf}
    \caption{Question difficulty versus controversy scatter plot. Each point represents a question, with x-axis showing average score (lower = harder) and y-axis showing score standard deviation (higher = more judge disagreement). Points are colored by question category.}
    \label{fig:question-autopsy}
\end{figure}

%% FIGURE 17: Radar Chart (multi-dimensional summary)
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/fig17_radar_chart.pdf}
    \caption{Multi-dimensional model comparison across five normalized metrics: Peer Score, Consistency (inverse of score variance), Speed (inverse of response time), Humility (inverse of self-bias), and Strictness (inverse of judge generosity). All metrics scaled to 0--1.}
    \label{fig:radar-chart}
\end{figure}


%% ============================================================================
%% CROSS-REFERENCE GUIDE
%% ============================================================================
%% Usage: Figure~\ref{fig:peer-rankings} shows...
%%
%% Label                 | Figure | Section
%% ----------------------|--------|------------
%% fig:pipeline          | 1      | Methodology
%% fig:peer-rankings     | 4      | Results
%% fig:cross-eval        | 5      | Results
%% fig:peer-score-speed  | 6      | Results
%% fig:self-bias         | 10     | Discussion
%% fig:name-bias         | 11     | Discussion
%% fig:position-bias     | 12     | Discussion
%% fig:judge-generosity  | 13     | Discussion
%% fig:judge-vs-peer     | 14     | Discussion
%% fig:judge-agreement   | 15     | Discussion
%% fig:question-autopsy  | 16     | Discussion
%% fig:radar-chart       | 17     | Discussion
